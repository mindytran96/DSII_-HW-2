---
title: "P8106- Homework #2"
author: 'Mindy Tran (mnt2130)'
date: "3/5/2023"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(caret)
library(GGally)
library(gridExtra)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center")
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
This reads the CSV file, cleans the variables names and eliminates any missing data.

```{r data import and cleaning}
set.seed(2132)
college_data = read_csv("./Data/College.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  relocate("outstate", .after = "grad_rate") %>% 
  select(-college)
```

This next step will partition the data into training (80% of data) and test (20%) data sets and create matrices of the training and testing data frame for further analysis. 

```{r partition}
indexTrain = createDataPartition(y = college_data$outstate,
                                 p = 0.8,
                                 list = FALSE)
training_df = college_data[indexTrain, ]
testing_df = college_data[-indexTrain, ]

x_train = model.matrix(outstate~.,training_df)[, -1]
y_train = training_df$outstate

x_test <- model.matrix(outstate~.,testing_df)[, -1]
y_test <- testing_df$outstate
```


## Part A: Smoothing Spline Model

Fit smoothing spline models using `perc_alumni` as the only predictor of `Outstate` for a range of degrees of freedom. 

``` {r smooth spline}

# This code fits a smoothing spline model for perc_alumni as a predictor of outstate
fit_ss = smooth.spline(training_df$perc_alumni, training_df$outstate)

# This outputs the optimal degrees of freedom from the cross-validation
fit_ss$df

alumni_grid = seq(from = 2, to = 64, by = 1)
pred_ss_grid = predict(fit_ss, x = alumni_grid)
pred_ss_grid_df = data.frame(predicted = pred_ss_grid$y,
                                        perc_alumni = alumni_grid)
p = ggplot(data = testing_df, aes(x = perc_alumni, y = outstate)) +
     geom_point(color = rgb(.2, .4, .2, .5)) 

p_line = p + geom_line(aes(x = perc_alumni, y = predicted), data = pred_ss_grid_df,
              color = rgb(.8, .1, .1, 1)) + theme_bw()

p_line

# Now we can use it on the test data

pred_ss_testing = predict(fit_ss, x = testing_df$perc_alumni)
pred_ss_testing_df = data.frame(predicted = pred_ss_testing$y,
                                           perc_alumni = testing_df$perc_alumni)
predicted_plot = p + geom_line(aes(x = perc_alumni, y = predicted), data = pred_ss_testing_df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()

```

This is the smoothing spline model using `perc_alumni` as the sole predictor for `outstate` using the optimal degrees of freedom (df=2.00024) outputted from the generalized cross validation. 

Now we will fit a smoothing spline model for a range of degrees of freedom: 

```{rrange of dfs }
# Smoothing Spline Model with Range of DFs 
spline_range = function(degree){
  
  spline_fit = smooth.spline(training_df$perc_alumni, training_df$outstate, df = degree)
  
  spline_pred = predict(spline_fit, x = alumni_grid)
  
  spline_df = data.frame(predicted = spline_pred$y,
                         perc_alumni = alumni_grid,
                         df = degree)
}

# Now we can run our spline function for DF values 2 through 10
datalist = list()
for (i in 2:10) {
  datalist[[i]] = spline_range(i)
}
all_data = do.call(rbind, datalist) %>% 
  as.data.frame()

# Plot for range of degree of freedom where red line represents optimal DF 

plot_range = p + 
  geom_line(aes(x = perc_alumni, y = predicted, group = df, color = df), data = all_data) + 
  geom_line(aes(x = perc_alumni, y = predicted), data = pred_ss_testing_df,
          color = rgb(.8, .1, .1, 1))

plot_range

```

When we overlay the models generated from different degree of freedoms, the model fittings are generally clustered and we see that with around 2-3  degree of freedoms, our model appears to fit more linear as represented by the purple lines but the lines (blue, green, yellow) starts to wiggle just a tiny bit when generated with higher degree of freedoms, suggesting slight potential over-fitting but not too much. 


## Part B: Generalized Additive Model

```{r}
set.seed(2132)
ctrl1 = trainControl(method = "cv", number = 10)

sapply(x_train %>% as.data.frame(), n_distinct)

#None of the predictors take on less than 10 values, thus we can proceed using the caret package, since we ensured that it won't result in loss of flexibility

# We can now run GAM using the caret package and use automatic feature selection
gam = train(x_train, y_train,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp",
                                      select = c(TRUE, FALSE)),
                trControl = ctrl1)

# This outputs the parameters which fit the best model
gam$bestTune
gam$finalModel


# This outputs the final model with the coefficients 
summary(gam)

# Plot of the GAM Model 

par(mar = c(1,1,1,1))
par(mfrow = c(4, 4))
plot(gam$finalModel, residuals = TRUE, all.terms = TRUE, shade = TRUE, shade.col = 2)

# Now we calculate the training and test MSE and RMSE of the optimized model 

#training MSE
gam_train_MSE = mean((y_train - predict(gam))^2)
gam_train_MSE

gam_train_RMSE = sqrt(gam_train_MSE)
gam_train_RMSE

#test MSE

test_pred = predict(gam, x_test)

gam_test_MSE = mean((y_test - test_pred)^2)
gam_test_MSE

gam_test_RMSE = sqrt(gam_test_MSE)
gam_test_RMSE

```

The optimal GAM model does include all predictors: 
s(perc_alumni) + s(terminal) + s(books) + s(top10perc) + s(grad_rate) + s(ph_d) + s(top25perc) + s(s_f_ratio) + s(personal) + s(p_undergrad) + s(enroll) + s(room_board) + s(accept) + s(f_undergrad) + s(apps) + s(expend). 

6 variables: terminal, top10perc, top25perc, personal, p_undergrad, enroll, all have 1 degree of freedom, which corresponds by their
straight line and confirmed as such in our plots. Variables with 2 degrees of freedom are incorporated as quadratics, while variables with 3 DFs are incorporated as cubic functions. perc_alumni, enroll, room_board, f_undergrad, and expend are the most significant smooth terms. 

Generating a model using all predictors, the optimized model when used on the *training data* obtains MSE = 2251375 with RMSE= 1500.458 and when used on the *test data* obtains MSE= 3364712 and an RMSE= 1834.315.


## Part D: Multivariate Adaptive Regression Spline Model










